

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 6.2 Diagonalizing a Matrix &mdash; LinearAlgebra  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 6.3 Systems of Differential Equations" href="chap6-3.html" />
    <link rel="prev" title="Chapter 6.1 Introduction to Eigenvalues" href="chap6-1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> LinearAlgebra
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chap1/index1.html">Chapter 1 Introduction to Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap2/index2.html">Chapter 2 Solving Linear Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap3/index3.html">Chapter 3 Vector Spaces and Subspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap4/index4.html">Chapter 4 Orthogonality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap5/index5.html">Chapter 5 Determinants</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index6.html">Chapter 6 Eigenvalues and Eigenvectors</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap6-1.html">Chapter 6.1 Introduction to Eigenvalues</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 6.2 Diagonalizing a Matrix</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#similar-matrices-same-eigenvalues">Similar Matrices: Same Eigenvalues</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fibonacci-numbers">Fibonacci numbers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#matrix-powers-a-k">Matrix Powers <span class="math notranslate nohighlight">\(A^k\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#nondiagonalizable-matrices-optional">Nondiagonalizable Matrices (Optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap6-3.html">Chapter 6.3 Systems of Differential Equations</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap6-4.html">Chapter 6.4 Symmetric Matrices</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap6-5.html">Chapter 6.5 Positive Definite Matrices</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">LinearAlgebra</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index6.html">Chapter 6 Eigenvalues and Eigenvectors</a> &raquo;</li>
        
      <li>Chapter 6.2 Diagonalizing a Matrix</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/chap6/chap6-2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\vds}{\vdots}
\newcommand{\lv}{\lVert}
\newcommand{\rv}{\rVert}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\a}{\boldsymbol{a}}
\newcommand{\b}{\boldsymbol{b}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\i}{\boldsymbol{i}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\p}{\boldsymbol{p}}
\newcommand{\q}{\boldsymbol{q}}
\newcommand{\u}{\boldsymbol{u}}
\newcommand{\v}{\boldsymbol{v}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}\\\newcommand{\A}{\boldsymbol{A}}
\newcommand{\B}{\boldsymbol{B}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\N}{\boldsymbol{N}}\\\newcommand{\R}{\boldsymbol{\mathrm{R}}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\Lambda}\\\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-6-2-diagonalizing-a-matrix">
<h1>Chapter 6.2 Diagonalizing a Matrix<a class="headerlink" href="#chapter-6-2-diagonalizing-a-matrix" title="Permalink to this headline">Â¶</a></h1>
<p>When <span class="math notranslate nohighlight">\(\x\)</span> is an eigenvector, multiplication by <span class="math notranslate nohighlight">\(A\)</span> is just
multiplication by a number <span class="math notranslate nohighlight">\(\ld\)</span>: <span class="math notranslate nohighlight">\(A\x=\ld\x\)</span>.</p>
<p>The point of this section is very direct.
<strong>The matrix</strong> <span class="math notranslate nohighlight">\(A\)</span> <strong>turns into a diagonal matrix</strong> <span class="math notranslate nohighlight">\(\Ld\)</span> <strong>when we use the eigenvectors properly</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Diagonalization</strong>: Suppose the <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> has
<span class="math notranslate nohighlight">\(n\)</span> linearly independent eigenvectors <span class="math notranslate nohighlight">\(\x1,\cds,\x_n\)</span>.
Put them into the columns of an <strong>eigenvector matrix</strong> <span class="math notranslate nohighlight">\(X\)</span>.
Then <span class="math notranslate nohighlight">\(X\im AX\)</span> is the <strong>eigenvalue matrix</strong> <span class="math notranslate nohighlight">\(\Ld\)</span>:</p>
<ul class="simple">
<li><p><strong>Eigenvector matrix</strong> <span class="math notranslate nohighlight">\(X\)</span>, <strong>Eigenvalue matrix</strong> <span class="math notranslate nohighlight">\(\Ld\)</span>:
<span class="math notranslate nohighlight">\(X\im AX=\Ld=\bb \ld_1\\&amp;\dds\\&amp;&amp;\ld_n \eb\)</span>.</p></li>
</ul>
</div>
<p>The matrix <span class="math notranslate nohighlight">\(A=\bb 1&amp;5\\0&amp; 6\eb\)</span> is triangular so its eigenvalues are on the diagonal: <span class="math notranslate nohighlight">\(\ld_1=1,\ld_2=6\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Eigenvectors go into</strong> <span class="math notranslate nohighlight">\(X\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bb 1\\0 \eb\bb 1\\1 \eb\)</span>.</p></li>
</ul>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}\bb 1&amp;-1\\0&amp;1 \eb\bb 1&amp;5\\0&amp;6 \eb\bb 1&amp;1\\0&amp;1 \eb=\bb 1&amp;0\\0&amp;6 \eb.\end{split}\]</div>
<p>In other words <span class="math notranslate nohighlight">\(A=X\Ld X\im\)</span>.
Then <span class="math notranslate nohighlight">\(A^2=X\Ld X\im X\Ld X\im=X\Ld^2X\im\)</span>.</p>
<p><span class="math notranslate nohighlight">\(A^2\)</span> <strong>has the same eigenvectors in</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>and squared eigenvalues in</strong> <span class="math notranslate nohighlight">\(\Ld^2\)</span>.</p>
<p><strong>Why is</strong> <span class="math notranslate nohighlight">\(AX=X\Ld\)</span>?
<span class="math notranslate nohighlight">\(A\)</span> multiplies its eigenvectors, which are the columns of <span class="math notranslate nohighlight">\(X\)</span>.
The first column of <span class="math notranslate nohighlight">\(AX\)</span> is <span class="math notranslate nohighlight">\(A\x_1\)</span>.
That is <span class="math notranslate nohighlight">\(\ld_1\x_1\)</span>.
Each column of <span class="math notranslate nohighlight">\(X\)</span> is multiplied by its eigenvalue:</p>
<p><span class="math notranslate nohighlight">\(A\)</span> <strong>times</strong> <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}AX=A\bb \\\ \x_1&amp;\cds&amp;\x_n \\\ \eb=\bb \\\ \ld_1\x_1&amp;\cds&amp;\ld_n\x_n \\\ \eb.\end{split}\]</div>
<p>The trick is to split thsis matrix <span class="math notranslate nohighlight">\(AX\)</span> into <span class="math notranslate nohighlight">\(X\)</span> times <span class="math notranslate nohighlight">\(\Ld\)</span>:</p>
<p><span class="math notranslate nohighlight">\(X\)</span> <strong>times</strong> <span class="math notranslate nohighlight">\(\Ld\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bb \\\ \ld_1\x_1&amp;\cds&amp;\ld_n\x_n \\\ \eb=\bb \\\ \x_1&amp;\cds&amp;\x_n \\\ \eb\bb \ld_1\\&amp;\dds\\&amp;&amp;\ld_n \eb=X\Ld.\end{split}\]</div>
<p>Keep those matrices in the right order.
Then <span class="math notranslate nohighlight">\(\ld_1\)</span> multiplies the first column <span class="math notranslate nohighlight">\(\x_1\)</span>, as shown.
The diagonalization is complete, and we can write <span class="math notranslate nohighlight">\(AX=X\Ld\)</span> in two good ways:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(AX=X\Ld\)</span> is <span class="math notranslate nohighlight">\(X\im AX=\Ld\)</span> or <span class="math notranslate nohighlight">\(A=X\Ld X\im\)</span>.</p>
</div>
<p>The matrix <span class="math notranslate nohighlight">\(X\)</span> has an inverse, because its columns (the eigenvectors of
<span class="math notranslate nohighlight">\(A\)</span>) were assumede to be linearly independent.
<em>Without</em> <span class="math notranslate nohighlight">\(n\)</span> <em>independent eigenvectors, we canât diagonalize</em>.</p>
<p><span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\Ld\)</span> have the same eigenvalues <span class="math notranslate nohighlight">\(\ld_1,\cds,\ld_n\)</span>.
The eigenvectors are different.
The <span class="math notranslate nohighlight">\(k\)</span> which is easy to compute:</p>
<div class="math notranslate nohighlight">
\[A^k=(X\Ld X\im)(X\Ld X\im)\cds(X\Ld X\im)=X\Ld^kX\im.\]</div>
<p><strong>Powers of</strong> <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bb 1&amp;5\\0&amp;6 \eb^k=\bb 1&amp;1\\0&amp;1 \eb\bb 1\\&amp;6^k \eb\bb 1&amp;-1\\0&amp;1 \eb=\bb 1&amp;6^k-1\\0&amp;6^k \eb=A^k.\end{split}\]</div>
<p><strong>Remark 1</strong>: Suppose the eigenvalues <span class="math notranslate nohighlight">\(\ld_1,\cds,\ld_n\)</span> are all different.
Then it is automatic that the eigenvectors <span class="math notranslate nohighlight">\(\x_1,cds,\x+n\)</span> are independent.
The eigenvector matrix <span class="math notranslate nohighlight">\(X\)</span> will be <em>invertible</em>.
<strong>Any matrix that has no repeated eigenvalues can be diagonalized</strong>.</p>
<p><strong>Remark 2</strong>: <strong>We can multiply eigenvectors by any nonzero constants</strong>.
<span class="math notranslate nohighlight">\(A(c\x)=\ld(c\x)\)</span> is still true.</p>
<p><strong>Remark 3</strong>: The eigenvectors in <span class="math notranslate nohighlight">\(X\)</span> come in the same order as the eigenvalues in <span class="math notranslate nohighlight">\(Ld\)</span>.
To reverse the order in <span class="math notranslate nohighlight">\(\Ld\)</span>, put the eigenvector <span class="math notranslate nohighlight">\((1,1)\)</span> before <span class="math notranslate nohighlight">\((1,0)\)</span> in <span class="math notranslate nohighlight">\(X\)</span>:</p>
<blockquote>
<div><p><strong>New order</strong> :math;`6,1`:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bb 0&amp;1\\1&amp;-1 \eb\bb 1&amp;5\\0&amp;6 \eb\bb 1&amp;1\\1&amp;0 \eb=\bb 6&amp;0\\0&amp;1 \eb=\Ld_{\rm{new}}.\end{split}\]</div>
<p>To diagonalize <span class="math notranslate nohighlight">\(A\)</span> we <em>must</em> use an eigenvector matrix.
From <span class="math notranslate nohighlight">\(X\im AX=\Ld\)</span> we know that <span class="math notranslate nohighlight">\(AX=X\Ld\)</span>.
Suppose the first column of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\x\)</span>.
Then the first columns of <span class="math notranslate nohighlight">\(AX\)</span> and <span class="math notranslate nohighlight">\(X\Ld\)</span> are <span class="math notranslate nohighlight">\(A\x\)</span> and <span class="math notranslate nohighlight">\(\ld_1\x\)</span>.
For those to be equal, <span class="math notranslate nohighlight">\(\x\)</span> must be an eigenvector.</p>
</div></blockquote>
<p><strong>Remark 4</strong> (repeated warning for repeated eigenvalues): Some matrices have too few eigenvectors.
<em>Those matrices cannot be diagonalized</em>.</p>
<blockquote>
<div><p><strong>Not diagnoalizable</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A=\bb 1&amp;-1\\1&amp;-1 \eb\quad \rm{and} \quad B=\bb 0&amp;1\\0&amp;1 \eb.\end{split}\]</div>
<p>Their engenvalues happen to be 0 and 0.
Nothing is special about <span class="math notranslate nohighlight">\(\ld=0\)</span>, the problem is the repetition of <span class="math notranslate nohighlight">\(\ld\)</span>.
All eigenvectors of the first matrix are multiples of <span class="math notranslate nohighlight">\((1,1)\)</span>:</p>
<p><strong>Only one line of eigenvectors</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A\x=0\x \quad\rm{means}\quad \bb 1&amp;-1\\1&amp;-1 \eb\bb \\\ \x\\\ \eb=\bb 0\\0 \eb\quad\rm{and}\quad \x=c\bb 1\\1 \eb.\end{split}\]</div>
<p>There is no second eigenvector, so this unusual matrix <span class="math notranslate nohighlight">\(A\)</span> cannot be diagonalized.</p>
<ul class="simple">
<li><p><strong>Invertibility</strong> is concerned with the <strong>eigenvalues</strong> (<span class="math notranslate nohighlight">\(\ld=0\)</span> or <span class="math notranslate nohighlight">\(\ld\neq 0\)</span>).</p></li>
<li><p><strong>Diagonalizability</strong> is concerned with the <strong>eigenvectors</strong> (too few or enough for <span class="math notranslate nohighlight">\(X\)</span>).</p></li>
</ul>
<p>Each eigenvalue has at least one eigenvector.
<span class="math notranslate nohighlight">\(A-\ld I\)</span> is singular.
If <span class="math notranslate nohighlight">\((A-\ld I)\x=\0\)</span> leads you to <span class="math notranslate nohighlight">\(\x=\0\)</span>, <span class="math notranslate nohighlight">\(\ld\)</span> is <em>not</em> an eigenvalue.
Look for a mistake in solving <span class="math notranslate nohighlight">\(\det(A-\ld I)=0\)</span>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>Eigenvectors for</strong> <span class="math notranslate nohighlight">\(n\)</span> <strong>different</strong> <span class="math notranslate nohighlight">\(\ld\)</span><strong>âs are independent</strong>.
<strong>Then we can diagonalize</strong> <span class="math notranslate nohighlight">\(A\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Independent</strong> <span class="math notranslate nohighlight">\(\x\)</span> <strong>from different</strong> <span class="math notranslate nohighlight">\(\ld\)</span>: Eigenvectors
<span class="math notranslate nohighlight">\(\x_1,\cds,\x_j\)</span> that correspond to distinct (all different)
eigenvalues are linearly independent.
An <span class="math notranslate nohighlight">\(n\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix that has <span class="math notranslate nohighlight">\(n\)</span> different
eigenvalues (no repeated <span class="math notranslate nohighlight">\(\ld\)</span>âs) must be diagonalizable.</p>
</div>
<p><strong>Proof</strong>: Suppose <span class="math notranslate nohighlight">\(c_1\x_1+c_2\x_2=\0\)</span>.
Multiply by <span class="math notranslate nohighlight">\(A\)</span> to find <span class="math notranslate nohighlight">\(c_1\ld_1\x_1+c_2\ld_2\x_2=\0\)</span>.
Multiply by <span class="math notranslate nohighlight">\(\ld_2\)</span> to find <span class="math notranslate nohighlight">\(c_1\ld_2\x_1+c_2\ld_2\x_2=\0\)</span>.
Now subtract one from the other: Subtraction leaves <span class="math notranslate nohighlight">\((\ld_1-\ld_2)c_1\x_1=\0\)</span>.
Therefore <span class="math notranslate nohighlight">\(c_1=0\)</span>.</p>
<blockquote>
<div><p>Since the <span class="math notranslate nohighlight">\(\ld\)</span>âs are different and <span class="math notranslate nohighlight">\(\x_1\neq\0\)</span>, we are forced to the conclusion that <span class="math notranslate nohighlight">\(c_1=0\)</span>.
Similarly <span class="math notranslate nohighlight">\(c_2=0\)</span>.
Only the combination with <span class="math notranslate nohighlight">\(c_1=c_2=0\)</span> gives <span class="math notranslate nohighlight">\(c_1\x_1+c_2\x_2=\0\)</span>.
So the eigenvectors <span class="math notranslate nohighlight">\(\x_1\)</span> and <span class="math notranslate nohighlight">\(\x_2\)</span> must be independent.</p>
</div></blockquote>
<p>This proof extends directly to <span class="math notranslate nohighlight">\(j\)</span> eigenvectors.
Suppose that <span class="math notranslate nohighlight">\(c_1\x_1+\cds+c_j\x_j=\0\)</span>,
Multiply by <span class="math notranslate nohighlight">\(A\)</span>, multiply by <span class="math notranslate nohighlight">\(\ld_j\)</span> and subtract.
This multiplies <span class="math notranslate nohighlight">\(\x_j\)</span> by <span class="math notranslate nohighlight">\(\ld_j-\ld_j=0\)</span>, and <span class="math notranslate nohighlight">\(\x_j\)</span> is gone.
Now multiply by <span class="math notranslate nohighlight">\(A\)</span> and by <span class="math notranslate nohighlight">\(\ld_{j-1}\)</span> and subtract.
This removes <span class="math notranslate nohighlight">\(\x_{j-1}\)</span>.
Eventually only <span class="math notranslate nohighlight">\(\x_1\)</span> is left: We reach
<span class="math notranslate nohighlight">\((\ld_1-\ld_2)\cds(\ld_1-\ld_j)c_1\x_1=\0\)</span> which forces <span class="math notranslate nohighlight">\(c_1=0\)</span>.</p>
<p>Similarly every <span class="math notranslate nohighlight">\(c_i=0\)</span>.
When the <span class="math notranslate nohighlight">\(\ld\)</span>âs are all different, the eigenvectors are independent.
A full set of eigenvectors can go into the columns of the eigenvector matrix <span class="math notranslate nohighlight">\(X\)</span>.</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Question</strong>: When does <span class="math notranslate nohighlight">\(A^k\rightarrow zero\ matrix\)</span>?
<strong>Answer</strong>: All <span class="math notranslate nohighlight">\(|\ld|&lt;1\)</span>.</p>
</div>
<div class="section" id="similar-matrices-same-eigenvalues">
<h2>Similar Matrices: Same Eigenvalues<a class="headerlink" href="#similar-matrices-same-eigenvalues" title="Permalink to this headline">Â¶</a></h2>
<p>Suppose the eigenvalue matrix <span class="math notranslate nohighlight">\(\Ld\)</span> is fixed.
As we change the eigenvector matrix <span class="math notranslate nohighlight">\(X\)</span>, we get a whole family of
different matrices <span class="math notranslate nohighlight">\(A=X\Ld X\im\)</span>â<em>all with the same eigenvalues in</em>
<span class="math notranslate nohighlight">\(\Ld\)</span>.
All those matrices <span class="math notranslate nohighlight">\(A\)</span> (with the same <span class="math notranslate nohighlight">\(\Ld\)</span>) are called <em>similar*</em>.</p>
<p>This idea extends to matrices that canât be diagonalized.
Again we choose one constant matrix <span class="math notranslate nohighlight">\(C\)</span> (not necessariily <span class="math notranslate nohighlight">\(\Ld\)</span>).
And we look at the whole family of matrices <span class="math notranslate nohighlight">\(A=BCB\im\)</span>, allowing all invertible matrices <span class="math notranslate nohighlight">\(B\)</span>.
Again those matrices <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(C\)</span> are called <strong>similar</strong>.</p>
<p>We are using <span class="math notranslate nohighlight">\(C\)</span> instead of <span class="math notranslate nohighlight">\(\Ld\)</span> because <span class="math notranslate nohighlight">\(C\)</span> might not be diagonal.
We are using <span class="math notranslate nohighlight">\(B\)</span> instead of <span class="math notranslate nohighlight">\(X\)</span> because the columns of <span class="math notranslate nohighlight">\(B\)</span> might not be eigenvectors.
We only require that <span class="math notranslate nohighlight">\(B\)</span> is invertibleâits columns can cantain any basis for <span class="math notranslate nohighlight">\(\R^n\)</span>.
<strong>Similar matrices</strong> <span class="math notranslate nohighlight">\(A\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(C\)</span> <strong>have the same eigenvalues</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>All the matrices</strong> <span class="math notranslate nohighlight">\(A=BCB\im\)</span> <strong>are âsimilarâ</strong>.
<strong>They all share the eigenvalues of</strong> <span class="math notranslate nohighlight">\(C\)</span>.</p>
</div>
<p><strong>Proof</strong>: Suppose <span class="math notranslate nohighlight">\(C\x=\ld\x\)</span>.
Then <span class="math notranslate nohighlight">\(BCB\im\)</span>ld` with the new eigenvector <span class="math notranslate nohighlight">\(B\x\)</span>:</p>
<div class="math notranslate nohighlight">
\[Same\ \ld\quad(BCB\im)(B\x)=BC\x=B\ld\x=\ld(B\x).\]</div>
<p>A fixed matrix <span class="math notranslate nohighlight">\(C\)</span> produces a family of similar matrices <span class="math notranslate nohighlight">\(BCB\im\)</span>, allowing all <span class="math notranslate nohighlight">\(B\)</span>.
When <span class="math notranslate nohighlight">\(C\)</span> is the identity matrix, the âfamilyâ is very small.
The only member is <span class="math notranslate nohighlight">\(BIB\im=I\)</span>.
The identity matrix is the only diagonalizable matrix with all eigenvalues <span class="math notranslate nohighlight">\(\ld=1\)</span>.</p>
<p>The family is larger when <span class="math notranslate nohighlight">\(\ld=1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> <em>with only one eigenvector</em> (not diagonalizable).
The simpliest <span class="math notranslate nohighlight">\(C\)</span> is the <em>Jordan form</em>.
All the similar <span class="math notranslate nohighlight">\(A\)</span>âs have two parameters <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(s\)</span>, not
both zero: always determinant = 1 and trace = 2.</p>
<div class="math notranslate nohighlight">
\[\begin{split}C=\bb 1&amp;1\\0&amp;1 \eb=\rm{\ Jordan\ form\ gives\ }A=BCB\im=\bb 1-rs&amp;r^2\\-s^2&amp;1+rs \eb.\end{split}\]</div>
</div>
<div class="section" id="fibonacci-numbers">
<h2>Fibonacci numbers<a class="headerlink" href="#fibonacci-numbers" title="Permalink to this headline">Â¶</a></h2>
<p><strong>Every new Fibonacci number is the sum of the two previous</strong> <span class="math notranslate nohighlight">\(F\)</span><strong>âs</strong>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>The sequence</strong> <span class="math notranslate nohighlight">\(0,1,1,2,3,5,8,13,\cds\)</span> <strong>comes from</strong> <span class="math notranslate nohighlight">\(F_{k+2}=F_{k+1}+F_k\)</span>.</p>
</div>
<p><strong>Problem: Find the Fibonacci number</strong> <span class="math notranslate nohighlight">\(F_{100}\)</span>: The slow way is to apply the rule <span class="math notranslate nohighlight">\(F_{k+2}=F_{k+1}+F_k\)</span>.
Linear algebra gives a better way.</p>
<p>The key is to begin with a matrix equation <span class="math notranslate nohighlight">\(\u_{k+1}=A\u_k\)</span>.
That is a <em>one-step</em> rule for vectors, while Fibonacci gave a two-step rule for scalars.
We match those rules by putting two Fibonacci numbers into a vector.
Then you will see the matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Let <span class="math notranslate nohighlight">\(\u_k=\bb F_{k+1}\\F_k \eb\)</span>.
The rule <span class="math notranslate nohighlight">\(\begin{matrix}F_{k+2}=F_{k+1}+F_k\\F_{k+1}=F_{k+1}\quad\quad
\end{matrix}\)</span> is <span class="math notranslate nohighlight">\(\u_{k+1}=\bb 1&amp;1\\1&amp;0 \eb\u_k\)</span>.</p>
</div>
<p><strong>Every step multiplies by</strong> <span class="math notranslate nohighlight">\(A=\bb 1&amp;1\\1&amp;0 \eb\)</span>.
After 100 steps we reach <span class="math notranslate nohighlight">\(\u_{100}=A^{100}\u_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\u_0=\bb 1\\0 \eb, \u_1=\bb 1\\1 \eb, \u_2=\bb 2\\1 \eb, \u_3=\bb 3\\2 \eb, \cds, \u_{100}=\bb F_{101}\\F_{100} \eb.\end{split}\]</div>
<p>Subtract <span class="math notranslate nohighlight">\(\ld\)</span> from the diagonal of <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A=\ld I=\bb 1-\ld&amp;1\\1&amp;-\ld \eb\quad\rm{leads\ to}\quad\det(A-\ld I)=\ld^2-\ld-1.\end{split}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Eigenvalues</strong>: <span class="math notranslate nohighlight">\(\dp\ld_1=\frac{1+\sqrt{5}}{2}\approx 1.618\)</span> and
<span class="math notranslate nohighlight">\(\dp\ld_2=\frac{1-\sqrt{5}}{2}\approx -.618\)</span>.</p>
</div>
<p>These eigenvalues lead to eigenvectors <span class="math notranslate nohighlight">\(\x_1=(\ld_1,1)\)</span> and <span class="math notranslate nohighlight">\(\x_2=(\ld_2,1)\)</span>.
Step 2 finds the combination of those eigenvectors that gives <span class="math notranslate nohighlight">\(\u_0=(1,0)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bb 1\\0 \eb=\frac{1}{\ld_1-\ld_2}\left(\bb\ld_1\\1\eb-\bb\ld_2\\1\eb\right)
\quad\rm{or}\quad\u_0=\frac{\x_1-\x_2}{\ld_1-\ld_2}.\end{split}\]</div>
<p>Step 3 multiplies <span class="math notranslate nohighlight">\(u_0\)</span> by <span class="math notranslate nohighlight">\(A^{100}\)</span> to find <span class="math notranslate nohighlight">\(u_{100}\)</span>.
The eigenvectors <span class="math notranslate nohighlight">\(\x_1\)</span> and <span class="math notranslate nohighlight">\(\x_2\)</span> stay separate!
They are multiplied by <span class="math notranslate nohighlight">\((\ld_1)^{100}\)</span> and <span class="math notranslate nohighlight">\((\ld_2)^{100}\)</span>:</p>
<p><strong>100 steps from</strong> <span class="math notranslate nohighlight">\(\u_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\u_{100}=\frac{(\ld_1)^{100}\x_1-(\ld_2)^{100}\x_2}{\ld_1-\ld_2}.\]</div>
<p>We want <span class="math notranslate nohighlight">\(F_{100}=\)</span> second component of <span class="math notranslate nohighlight">\(\u_{100}\)</span>.
The second components of <span class="math notranslate nohighlight">\(\x_1\)</span> and <span class="math notranslate nohighlight">\(\x_2\)</span> are 1.
The difference between <span class="math notranslate nohighlight">\(\ld_1=(1+\sqrt{5})/2\)</span> and <span class="math notranslate nohighlight">\(\ld_2=(1-\sqrt{5})/2\)</span> is <span class="math notranslate nohighlight">\(\sqrt{5}\)</span>.
And <span class="math notranslate nohighlight">\(\ld_2^{100}\approx 0\)</span>.</p>
<p>100th Fibonacci number <span class="math notranslate nohighlight">\(=\dp\frac{\ld_1^{100}-\ld_2^{100}}{\ld_1-\ld_2}=\)</span>
nearest integer to <span class="math notranslate nohighlight">\(\dp\frac{1}{\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{100}\)</span>.</p>
<p>Every <span class="math notranslate nohighlight">\(F_k\)</span> is a whole number.
The ratio <span class="math notranslate nohighlight">\(F_{101}/F_{100}\)</span> must be very close to the limiting ratio <span class="math notranslate nohighlight">\((1+\sqrt{5})/2\)</span>.
This number is the â<em>golden mean</em>â.</p>
</div>
<div class="section" id="matrix-powers-a-k">
<h2>Matrix Powers <span class="math notranslate nohighlight">\(A^k\)</span><a class="headerlink" href="#matrix-powers-a-k" title="Permalink to this headline">Â¶</a></h2>
<p>Fibonacciâs example is a typical difference equation <span class="math notranslate nohighlight">\(\u_{k+1}=A\u_k\)</span>.
<strong>Each step multiplies by</strong> <span class="math notranslate nohighlight">\(A\)</span>.
The solution is <span class="math notranslate nohighlight">\(\u_k=A^k\u_0\)</span>.</p>
<p>The eigenvector matrix <span class="math notranslate nohighlight">\(X\)</span> produces <span class="math notranslate nohighlight">\(A=X\Ld X\im\)</span>.
This is a factorization of the matrix, like <span class="math notranslate nohighlight">\(A=LU\)</span> or <span class="math notranslate nohighlight">\(A=QR\)</span>.
The new factorization is perfectly suited to computing powers, because
<strong>every time</strong> <span class="math notranslate nohighlight">\(X\im\)</span> <strong>multiplies</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>we get</strong> <span class="math notranslate nohighlight">\(I\)</span>:</p>
<p><strong>Powers of</strong> <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[A^k\u_0=(X\Ld X\im)\cds(X\Ld X\im)\u_0=X\Ld^kX\im\u_0.\]</div>
<p>I will split <span class="math notranslate nohighlight">\(X\Ld^kX\im\u_0\)</span> into three steps that show how eigenvalues work:</p>
<ol class="arabic simple">
<li><p>Write <span class="math notranslate nohighlight">\(\u_0\)</span> as a combination <span class="math notranslate nohighlight">\(c_1\x_1+\cds+n_n\x_n\)</span> of the eigenvectors.
Then <span class="math notranslate nohighlight">\(\bs{c}=X\im\u_0\)</span>.</p></li>
<li><p>Multiply each eigenvector <span class="math notranslate nohighlight">\(\x_i\)</span> by <span class="math notranslate nohighlight">\((\ld_i)^k\)</span>.
Now we have <span class="math notranslate nohighlight">\(\Ld^kX\im\u_0\)</span>.</p></li>
<li><p>Add up the pieces <span class="math notranslate nohighlight">\(c_i(\ld_i)^k\x_i\)</span> to find the solution <span class="math notranslate nohighlight">\(\u_kA^k\u_0\)</span>.
This is <span class="math notranslate nohighlight">\(X\Ld^kX\im\u_0\)</span>.</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Solution for</strong> <span class="math notranslate nohighlight">\(\u_{k+1}=A\u_k\)</span>: <span class="math notranslate nohighlight">\(\u_k=A^k\u_0=c_1(\ld_1)^k\x_1+\cds+c_n(\ld_n)^k\x_n\)</span>.</p>
</div>
<p>In matrix language <span class="math notranslate nohighlight">\(A^k\)</span> equals <span class="math notranslate nohighlight">\((X\Ld X\im)^k\)</span> which is <span class="math notranslate nohighlight">\(X\)</span> times <span class="math notranslate nohighlight">\(\Ld^k\)</span> times <span class="math notranslate nohighlight">\(X\im\)</span>.
In Step 1, the eigenvectors in <span class="math notranslate nohighlight">\(X\)</span> lead to the <span class="math notranslate nohighlight">\(c\)</span>âs in the combination <span class="math notranslate nohighlight">\(\u_0=c_1\x_1+\cds+c_n\x_n\)</span>:</p>
<p><strong>Step 1</strong>: <span class="math notranslate nohighlight">\(\u_0=\bb \\\ \x_1&amp;\cds&amp;\x_n\\\ \eb\bb c_1\\\vds\\c_n \eb\)</span>.
This says that <span class="math notranslate nohighlight">\(\u_0=X\bs{c}\)</span>.</p>
<p>The coefficients in Step 1 are <span class="math notranslate nohighlight">\(\bs{c}=X\im\u_0\)</span>.
Then Step 2 multiplies by <span class="math notranslate nohighlight">\(\Ld^k\)</span>.
The final result <span class="math notranslate nohighlight">\(\u_k=\sum c_i(\ld_i)^k\x_i\)</span> in Step 3 is the product of
<span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(\Ld^k\)</span> and <span class="math notranslate nohighlight">\(X\im\u_0\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}A^k\u_0=X\Ld^kX\im\u_0=X\Ld^k\bs{c}=\bb \\\ \x_1&amp;\cds&amp;\x_n \\\ \eb
\bb (\ld_1)^k\\&amp;\dds\\&amp;&amp;(\ld_n)^k \eb\bb c_1\\\vds\\c_n \eb.\end{split}\]</div>
<p>The result is exactly <span class="math notranslate nohighlight">\(\u_k=c_1(\ld_1)^k\x_1+\cds+c_n(\ld_n)^k\x_n\)</span>.
It solves <span class="math notranslate nohighlight">\(\u_{k+1}=A\u_k\)</span>.</p>
</div>
<div class="section" id="nondiagonalizable-matrices-optional">
<h2>Nondiagonalizable Matrices (Optional)<a class="headerlink" href="#nondiagonalizable-matrices-optional" title="Permalink to this headline">Â¶</a></h2>
<p>Suppose <span class="math notranslate nohighlight">\(\ld\)</span> is an eigenvalue of <span class="math notranslate nohighlight">\(A\)</span>.
We discover that fact in two ways:</p>
<p><strong>1. Eigenvectors</strong> (geometric): There are nonzero solutions to <span class="math notranslate nohighlight">\(A\x=\ld\x\)</span>.</p>
<p><strong>2. Eigenvalues</strong> (algebraic): The determinant of <span class="math notranslate nohighlight">\(A-\ld I\)</span> is zero.</p>
<p>The number <span class="math notranslate nohighlight">\(\ld\)</span> may be a simple eigenvalue or a multiple eigenvalue, and we want to know its <strong>multiplicity</strong>.
Most eigenvalues have multiplicity <span class="math notranslate nohighlight">\(M=1\)</span> (simple eigenvalues).
Then there is a single line of eigenvectors, and <span class="math notranslate nohighlight">\(\det(A-\ld I)\)</span> does not have a double factor.</p>
<p>For exceptional matrices, an eigenvalue can be <strong>repeated</strong>.
Then there are two different ways to count its multiplicity.
Always GM <span class="math notranslate nohighlight">\(\leq\)</span> AM for each <span class="math notranslate nohighlight">\(\ld\)</span>:</p>
<ol class="arabic simple">
<li><p>(Geometric Multiplicity = GM): Count the <strong>independent eigenvectors</strong> for <span class="math notranslate nohighlight">\(\ld\)</span>.
The GM is the dimension of the nullspace of <span class="math notranslate nohighlight">\(A-\ld I\)</span>.</p></li>
<li><p>(Algebraic Multiplicity = AM): AM counts the <strong>repetitions of</strong> <span class="math notranslate nohighlight">\(\ld\)</span> among the eigenvalues.
Look at the <span class="math notranslate nohighlight">\(n\)</span> roots of <span class="math notranslate nohighlight">\(\det(A-\ld I)=0\)</span>.</p></li>
</ol>
<p>If <span class="math notranslate nohighlight">\(A\)</span> has <span class="math notranslate nohighlight">\(\ld=4,4,4\)</span>, then that eigenvalue has AM = 3 and GM = 1, 2, or 3.</p>
<p><strong>The shortage of eigenvectors when</strong> GM <strong>is below</strong> AM <strong>means that</strong> <span class="math notranslate nohighlight">\(A\)</span> <strong>is not diagonalizable</strong>.</p>
<hr class="docutils" />
<p><strong>If</strong> <span class="math notranslate nohighlight">\(A\)</span> <strong>is</strong> <span class="math notranslate nohighlight">\(m\)</span> <strong>by</strong> <span class="math notranslate nohighlight">\(n\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(B\)</span> <strong>is</strong>
<span class="math notranslate nohighlight">\(n\)</span> <strong>by</strong> <span class="math notranslate nohighlight">\(m\)</span>, <strong>then</strong> <span class="math notranslate nohighlight">\(AB\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(BA\)</span>
<strong>have same nonzero eigenvalues</strong>.</p>
<p><strong>Proof</strong>: Start with this identity between square matrices (easily checked).
The first and third matrices are inverses.
The âsize matrixâ shows the shapes of all blocks.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bb I&amp;-A\\0&amp;I \eb\bb AB&amp;0\\B&amp;0 \eb\bb I&amp;A\\0&amp;I \eb=\bb 0&amp;0\\B&amp;BA \eb\quad
\bb m \times m&amp;m \times n\\n \times m&amp;n \times n \eb\end{split}\]</div>
<p>This equation <span class="math notranslate nohighlight">\(D\im ED=F\)</span> says <span class="math notranslate nohighlight">\(F\)</span> <strong>is similar to</strong> <span class="math notranslate nohighlight">\(E\)</span>âthey have the same <span class="math notranslate nohighlight">\(m+n\)</span> eigenvalues.</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(E=\bb AB&amp;0\\B&amp;0 \eb\)</span> has the <span class="math notranslate nohighlight">\(m\)</span> eigenvalues of <span class="math notranslate nohighlight">\(AB\)</span>, plus <span class="math notranslate nohighlight">\(n\)</span> zeros</p>
<p><span class="math notranslate nohighlight">\(F=\bb 0&amp;0\\B&amp;BA \eb\)</span> has the <span class="math notranslate nohighlight">\(n\)</span> eigenvalues of <span class="math notranslate nohighlight">\(BA\)</span>, plus <span class="math notranslate nohighlight">\(m\)</span> zeros</p>
</div></blockquote>
<p>So <span class="math notranslate nohighlight">\(AB\)</span> and <span class="math notranslate nohighlight">\(BA\)</span> have the <strong>same eigenvalues</strong> except for <span class="math notranslate nohighlight">\(|n-m|\)</span> zeros.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap6-3.html" class="btn btn-neutral float-right" title="Chapter 6.3 Systems of Differential Equations" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap6-1.html" class="btn btn-neutral float-left" title="Chapter 6.1 Introduction to Eigenvalues" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>