

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 12.2 Covariance Matrices and Joint Probabilities &mdash; LinearAlgebra  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 12.3 Multivariate Gaussian and Weighted Least Squares" href="chap12-3.html" />
    <link rel="prev" title="Chapter 12.1 Mean, Variance, and Probability" href="chap12-1.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> LinearAlgebra
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chap1/index1.html">Chapter 1 Introduction to Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap2/index2.html">Chapter 2 Solving Linear Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap3/index3.html">Chapter 3 Vector Spaces and Subspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap4/index4.html">Chapter 4 Orthogonality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap5/index5.html">Chapter 5 Determinants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap6/index6.html">Chapter 6 Eigenvalues and Eigenvectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap7/index7.html">Chapter 7 The Singular Value Decomposition (SVD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap8/index8.html">Chapter 8 Linear Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap9/index9.html">Chapter 9 Complex Vectors and Matrices</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index12.html">Chapter 12 Linear Algebra in Probability &amp; Statistics</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="chap12-1.html">Chapter 12.1 Mean, Variance, and Probability</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 12.2 Covariance Matrices and Joint Probabilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-covariance-matrix-v-is-positive-semidefinite">The Covariance Matrix <span class="math notranslate nohighlight">\(V\)</span> is Positive Semidefinite</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-mean-and-variance-of-z-x-y">The Mean and Variance of <span class="math notranslate nohighlight">\(z=x+y\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-covariance-matrix-for-z-ax">The Covariance Matrix for <span class="math notranslate nohighlight">\(Z=AX\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-correlation-rho">The Correlation <span class="math notranslate nohighlight">\(\rho\)</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap12-3.html">Chapter 12.3 Multivariate Gaussian and Weighted Least Squares</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">LinearAlgebra</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index12.html">Chapter 12 Linear Algebra in Probability &amp; Statistics</a> &raquo;</li>
        
      <li>Chapter 12.2 Covariance Matrices and Joint Probabilities</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/chap12/chap12-2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\vds}{\vdots}
\newcommand{\lv}{\lVert}
\newcommand{\rv}{\rVert}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\a}{\boldsymbol{a}}
\newcommand{\b}{\boldsymbol{b}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\i}{\boldsymbol{i}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\p}{\boldsymbol{p}}
\newcommand{\q}{\boldsymbol{q}}
\newcommand{\u}{\boldsymbol{u}}
\newcommand{\v}{\boldsymbol{v}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}\\\newcommand{\A}{\boldsymbol{A}}
\newcommand{\B}{\boldsymbol{B}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\N}{\boldsymbol{N}}
\newcommand{\X}{\boldsymbol{X}}\\\newcommand{\R}{\boldsymbol{\mathrm{R}}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\Lambda}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\Sigma}
\newcommand{\th}{\theta}\\\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-12-2-covariance-matrices-and-joint-probabilities">
<h1>Chapter 12.2 Covariance Matrices and Joint Probabilities<a class="headerlink" href="#chapter-12-2-covariance-matrices-and-joint-probabilities" title="Permalink to this headline">¶</a></h1>
<p><span class="math notranslate nohighlight">\(p_{ij}=\)</span> <strong>Probability that experiment 1 produces</strong> <span class="math notranslate nohighlight">\(x_i\)</span> <strong>and experiment 2 produces</strong> <span class="math notranslate nohighlight">\(y_j\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Covariance</strong>: <span class="math notranslate nohighlight">\(\dp\sg_{12}=\sum_{\rm{all}}\sum_{i,j}p_{ij}(x_i-m_1)(y_j-m_2)\)</span>.</p>
</div>
<p><strong>Probability matrix</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}P=\bb p_{11}&amp;p_{12}\\p_{21}&amp;p_{22} \eb\end{split}\]</div>
<p>Notice the row sums <span class="math notranslate nohighlight">\(p_i\)</span> and column sums <span class="math notranslate nohighlight">\(P_i\)</span> and the total sum = 1.
Those numbers <span class="math notranslate nohighlight">\(p_1,p_2\)</span> and <span class="math notranslate nohighlight">\(P_1,P_2\)</span> are called the <strong>marginals</strong> of the matrix <span class="math notranslate nohighlight">\(P\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Zero covariance</strong> <span class="math notranslate nohighlight">\(\sg_{12}\)</span> <strong>for independent trials</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(V=\bb \sg_1^2&amp;0\\0&amp;\sg_2^2 \eb=\)</span> <strong>diagonal covariance matrix</strong>.</p></li>
</ul>
</div>
<p>Independent experiments have <span class="math notranslate nohighlight">\(\sg_{12}=0\)</span> because every <span class="math notranslate nohighlight">\(p_{ij}=(p_i)(p_j)\)</span> in equation:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\sg_{12}=\sum_i\sum_j(p_i)(p_j)(x_i-m_1)(y_j-m_2)=\\\bigg[\sum_i(p_i)(x_i-m_1)\bigg]\bigg[\sum_j(p_j)(y_j-m_2)\bigg]=[0][0].\end{aligned}\end{align} \]</div>
<p><strong>Always</strong> <span class="math notranslate nohighlight">\(\sg_1^2\sg_2^2\geq\sg_{12}^2\)</span>.
Thus <span class="math notranslate nohighlight">\(\sg_{12}\)</span> is <em>between</em> <span class="math notranslate nohighlight">\(-\sg_1\sg_2\)</span> <em>and</em> <span class="math notranslate nohighlight">\(\sg_1\sg_2\)</span>.
The covariance matrix <span class="math notranslate nohighlight">\(V\)</span> is <strong>positive (semi)definite</strong>.
This is an important fact about <span class="math notranslate nohighlight">\(M\)</span> by <span class="math notranslate nohighlight">\(M\)</span> covariance matrices for <span class="math notranslate nohighlight">\(M\)</span> experiments.</p>
<p>Note that the <strong>sample covariance matrix</strong> <span class="math notranslate nohighlight">\(S\)</span> from <span class="math notranslate nohighlight">\(N\)</span> trials is certainly semidefinite.
Every new sample <span class="math notranslate nohighlight">\(X\)</span> contributes to the <strong>sample mean</strong> <span class="math notranslate nohighlight">\(\bar{\X}\)</span> and to <span class="math notranslate nohighlight">\(S\)</span>.
Each term <span class="math notranslate nohighlight">\((X_i-\bar{\X})(X_i-\bar{\X})^T\)</span> is positive semidefinite and we just add to reach <span class="math notranslate nohighlight">\(S\)</span>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\dp\bar{\X}=\frac{X_1+\cds+X_N}{N}\)</span></p>
<p><span class="math notranslate nohighlight">\(\dp S=\frac{(X_1-\bar{\X})(X_1-\bar{\X})^T+\cds+(X_N-\bar{\X})(X_N-\bar{\X})^T}{N-1}\)</span></p>
</div>
<div class="section" id="the-covariance-matrix-v-is-positive-semidefinite">
<h2>The Covariance Matrix <span class="math notranslate nohighlight">\(V\)</span> is Positive Semidefinite<a class="headerlink" href="#the-covariance-matrix-v-is-positive-semidefinite" title="Permalink to this headline">¶</a></h2>
<p><strong>Total probability (all pairs) is 1</strong>:</p>
<div class="math notranslate nohighlight">
\[\sum_{\rm{all}}\sum_{i,j}p_{ij}=1.\]</div>
<p><strong>Row sum</strong> <span class="math notranslate nohighlight">\(p_i\)</span> <strong>of</strong> <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^np_{ij}=\rm{probability\ }p_i\rm{\ of\ }x_i\rm{\ in\ experiment\ 1}.\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Covariance matrix</strong> <span class="math notranslate nohighlight">\(V=\Sg\Sg V_{ij}\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dp V=\sum_{\rm{all}}\sum_{i,j}p_{ij}\bb (x_i-m_1)^2&amp;(x_i-m_1)(y_j-m_2)\\(x_i-m_1)(y_j-m_2)&amp;(y_j-m_2)^2\eb\)</span>.</p></li>
</ul>
</div>
<div class="math notranslate nohighlight">
\[V_{11}=\sum_{\rm{all}}\sum_{i,j}p_{ij}(x_j-m_1)^2=\sum_{\rm{all\ }i}(\rm{probability\ of\ }x_i)(x_i-m_1)^2=\sg_1^2.\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(V_{ij}\)</span> for each pair of outcomes <span class="math notranslate nohighlight">\(i,j\)</span> is <strong>positive semidefinite</strong>.
<span class="math notranslate nohighlight">\(V_{ij}\)</span> has diagonal entries <span class="math notranslate nohighlight">\(p_{ij}(x_i-m_1)^2\geq 0\)</span> and
<span class="math notranslate nohighlight">\(p_{ij}(y_j-m_2)^2\geq 0\)</span> and <span class="math notranslate nohighlight">\(\det(V_{ij})=0\)</span>.
That matrix <span class="math notranslate nohighlight">\(V_{ij}\)</span> has rank 1.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\bb (x_i-m_1)^2&amp;(x_i-m_1)(y_j-m_2)\\(x_i-m_1)(y_j-m_2)&amp;(y_j-m_2)^2\eb=
\bb x_i-m_1\\y_j-m_2 \eb\bb x_i-m_1&amp;y_j-m_2 \eb.\end{split}\]</div>
<p><em>Every matrix</em> <span class="math notranslate nohighlight">\(UU^T\)</span> <em>is positive semidefinite</em>.
Sothe whole matrix <span class="math notranslate nohighlight">\(V\)</span> (combining these matrices <span class="math notranslate nohighlight">\(UU^T\)</span> with weights
<span class="math notranslate nohighlight">\(p_{ij}\geq 0\)</span>) is <strong>at least semidefinite</strong>.</p>
<p><strong>The covariance matrix</strong> <span class="math notranslate nohighlight">\(V\)</span> <strong>is positive definite unless the experiments are dependent</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Covariance matrix</strong>: :math:<a href="#id1"><span class="problematic" id="id2">`</span></a>V=rm{E}[(X-bar{X})(X-bar{X})^T]</p>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\rm{Variance\ of\ }c^T\X=\rm{E}[(c^T\X-c^T\bar{\X})(c^T\X-c^T\bar{\X})^T]\\=c^T\rm{E}[(\X-\bar{\X})(\X-\bar{\X})]c=c^TVc\end{aligned}\end{align} \]</div>
<p><em>The variance of</em> <span class="math notranslate nohighlight">\(c^T\X\)</span> <em>can never be negative</em>.
So <span class="math notranslate nohighlight">\(c^TVc\geq 0\)</span>.
<em>The covariance matrix</em> <span class="math notranslate nohighlight">\(V\)</span> <em>is therefore positive semidefinite by the energy test</em> <span class="math notranslate nohighlight">\(c^TVc\geq 0\)</span>.</p>
<p><span class="math notranslate nohighlight">\(V\)</span> equals <span class="math notranslate nohighlight">\(Q\Ld Q^T\)</span> with eigenvalues <span class="math notranslate nohighlight">\(\ld_i\geq 0\)</span> and
orthonormal eigenvectors <span class="math notranslate nohighlight">\(\q_1\)</span> to <span class="math notranslate nohighlight">\(\q_M\)</span>.
<strong>Diagonalizing the covariance matrix means finding</strong> <span class="math notranslate nohighlight">\(M\)</span>
<strong>independent experiments as combinations of the original</strong> <span class="math notranslate nohighlight">\(M\)</span>
<strong>experiments</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Covariance matrix</strong>: <span class="math notranslate nohighlight">\(\dp V=\iiint p(x,y,z)UU^Tdxdydz\)</span> with
<span class="math notranslate nohighlight">\(U=\bb x-\bar{x}\\y-\bar{y}\\z-\bar{z} \eb\)</span>.</p>
</div>
<ul class="simple">
<li><p><strong>Independent variables</strong> <span class="math notranslate nohighlight">\(x,y,z\)</span>: <span class="math notranslate nohighlight">\(p(x,y,z)=p_1(x)p_2(y)p_3(z)\)</span>.</p></li>
<li><p><strong>Dependeent variables</strong> <span class="math notranslate nohighlight">\(x,y,z\)</span>: <span class="math notranslate nohighlight">\(p(x,y,z)=0\)</span> except when <span class="math notranslate nohighlight">\(cy+dy+ez=0\)</span>.</p></li>
</ul>
</div>
<div class="section" id="the-mean-and-variance-of-z-x-y">
<h2>The Mean and Variance of <span class="math notranslate nohighlight">\(z=x+y\)</span><a class="headerlink" href="#the-mean-and-variance-of-z-x-y" title="Permalink to this headline">¶</a></h2>
<p><strong>The sample mean of</strong> <span class="math notranslate nohighlight">\(z=x+y\)</span> <strong>is clearly</strong> <span class="math notranslate nohighlight">\(m_z=m_x+m_y\)</span>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Mean of sum = Sum of means</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dp\frac{1}{N}\sum_1^N(x_i+y_i)=\frac{1}{N}\sum_1^Nx_i+\frac{1}{N}\sum_1^Ny_i\)</span>.</p></li>
</ul>
</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\rm{E}[x+y]=\sum_i\sum_j p_{ij}(x_i+y_j)=\sum_i\sum_j p_{ij}x_i+\sum_i\sum_j p_{ij}y_j.\\\sum_i\sum_j p_{ij}x_i=\sum_i(p_{i1}+\cds+p_{iN})x_i=\sum_i p_ix_i=\rm{E}[x]\\\sum_i\sum_j p_{ij}y_j=\sum_j(p_{1j}+\cds+p_{nj})y_j=\sum_j p_jy_j=\rm{E}[y]\end{aligned}\end{align} \]</div>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\sg_z^2&amp;=\sum\sum p_{ij}(x_i+y_j-m_x-m_y)^2\\&amp;=\sum\sum p_{ij}(x_I-m_x)^2+\sum\sum p_{ij}(y_j-m_y)^2+2\sum\sum p_{ij}(x_i-m_x)(y_j-m_y)\end{aligned}\end{align} \]</div>
<p><strong>The variance of</strong> <span class="math notranslate nohighlight">\(z=x+y\)</span> <strong>is</strong> <span class="math notranslate nohighlight">\(\sg_z^2=\sg_x^2+\sg_y^2+2\sg_{xy}\)</span>.</p>
</div>
<div class="section" id="the-covariance-matrix-for-z-ax">
<h2>The Covariance Matrix for <span class="math notranslate nohighlight">\(Z=AX\)</span><a class="headerlink" href="#the-covariance-matrix-for-z-ax" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>The covariance matrix of</strong> <span class="math notranslate nohighlight">\(Z=AX\)</span> <strong>is</strong> <span class="math notranslate nohighlight">\(V_Z=AV_XA^T\)</span>.</p>
</div>
</div>
<div class="section" id="the-correlation-rho">
<h2>The Correlation <span class="math notranslate nohighlight">\(\rho\)</span><a class="headerlink" href="#the-correlation-rho" title="Permalink to this headline">¶</a></h2>
<p><strong>The new</strong> <span class="math notranslate nohighlight">\(X=x/\rho_x\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(Y=y/\rho_y\)</span> <strong>have variance</strong> <span class="math notranslate nohighlight">\(\sg_X^2=\sg_Y^2=1\)</span>.
<strong>The correlation of</strong> <span class="math notranslate nohighlight">\(x\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(y\)</span> <strong>is the covariance of</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Correlation</strong>: <span class="math notranslate nohighlight">\(\dp\rho_{xy}=\frac{\sg_{xy}}{\sg_x\sg_y}=\)</span>
<strong>covariance of</strong> <span class="math notranslate nohighlight">\(\dp\frac{x}{\sg_x}\)</span> <strong>and</strong>
<span class="math notranslate nohighlight">\(\dp\frac{y}{\rho_y}\)</span>.
Always <span class="math notranslate nohighlight">\(-1\leq\rho_{xy}\leq 1\)</span>.</p>
</div>
<p>Zero covariance gives zero correlation.
<em>Independent random variables produce</em> <span class="math notranslate nohighlight">\(\rho_{xy}=0\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(\sg_{xy}^2\leq\sg_x^2\sg_y^2\)</span>, then <span class="math notranslate nohighlight">\(\rho_{xy}^2\leq 1\)</span>.
Correlation near <span class="math notranslate nohighlight">\(\rho=+1\)</span> means strong dependence in the same direction.
Negative correlation means that <span class="math notranslate nohighlight">\(y\)</span> tends to be below its mean when <span class="math notranslate nohighlight">\(x\)</span> is above its mean.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap12-3.html" class="btn btn-neutral float-right" title="Chapter 12.3 Multivariate Gaussian and Weighted Least Squares" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="chap12-1.html" class="btn btn-neutral float-left" title="Chapter 12.1 Mean, Variance, and Probability" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>