

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Chapter 12.1 Mean, Variance, and Probability &mdash; LinearAlgebra  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 12.2 Covariance Matrices and Joint Probabilities" href="chap12-2.html" />
    <link rel="prev" title="Chapter 12 Linear Algebra in Probability &amp; Statistics" href="index12.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> LinearAlgebra
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chap1/index1.html">Chapter 1 Introduction to Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap2/index2.html">Chapter 2 Solving Linear Equations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap3/index3.html">Chapter 3 Vector Spaces and Subspaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap4/index4.html">Chapter 4 Orthogonality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap5/index5.html">Chapter 5 Determinants</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap6/index6.html">Chapter 6 Eigenvalues and Eigenvectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap7/index7.html">Chapter 7 The Singular Value Decomposition (SVD)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap8/index8.html">Chapter 8 Linear Transformations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chap9/index9.html">Chapter 9 Complex Vectors and Matrices</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index12.html">Chapter 12 Linear Algebra in Probability &amp; Statistics</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chapter 12.1 Mean, Variance, and Probability</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#variance-around-the-mean">Variance (around the mean)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#continuous-probability-distributions">Continuous Probability Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mean-and-variance-of-p-x">Mean and Variance of <span class="math notranslate nohighlight">\(p(x)\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#normal-distribution-bell-shaped-curve">Normal Distribution: Bell-shaped Curve</a></li>
<li class="toctree-l3"><a class="reference internal" href="#n-coin-flips-and-n-rightarrow-infty"><span class="math notranslate nohighlight">\(N\)</span> Coin Flips and <span class="math notranslate nohighlight">\(N\rightarrow \infty\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="#monte-carlo-estimation-methods">Monte Carlo Estimation Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#review-three-formulas-for-the-mean-and-the-variance">Review: Three Formulas for the Mean and the Variance</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="chap12-2.html">Chapter 12.2 Covariance Matrices and Joint Probabilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="chap12-3.html">Chapter 12.3 Multivariate Gaussian and Weighted Least Squares</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">LinearAlgebra</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index12.html">Chapter 12 Linear Algebra in Probability &amp; Statistics</a> &raquo;</li>
        
      <li>Chapter 12.1 Mean, Variance, and Probability</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/chap12/chap12-1.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\newcommand{\bs}{\boldsymbol}
\newcommand{\dp}{\displaystyle}
\newcommand{\rm}{\mathrm}
\newcommand{\pd}{\partial}\\\newcommand{\cd}{\cdot}
\newcommand{\cds}{\cdots}
\newcommand{\dds}{\ddots}
\newcommand{\vds}{\vdots}
\newcommand{\lv}{\lVert}
\newcommand{\rv}{\rVert}
\newcommand{\wh}{\widehat}\\\newcommand{\0}{\boldsymbol{0}}
\newcommand{\a}{\boldsymbol{a}}
\newcommand{\b}{\boldsymbol{b}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\i}{\boldsymbol{i}}
\newcommand{\j}{\boldsymbol{j}}
\newcommand{\p}{\boldsymbol{p}}
\newcommand{\q}{\boldsymbol{q}}
\newcommand{\u}{\boldsymbol{u}}
\newcommand{\v}{\boldsymbol{v}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}\\\newcommand{\A}{\boldsymbol{A}}
\newcommand{\B}{\boldsymbol{B}}
\newcommand{\C}{\boldsymbol{C}}
\newcommand{\N}{\boldsymbol{N}}
\newcommand{\X}{\boldsymbol{X}}\\\newcommand{\R}{\boldsymbol{\mathrm{R}}}\\\newcommand{\ld}{\lambda}
\newcommand{\Ld}{\Lambda}
\newcommand{\sg}{\sigma}
\newcommand{\Sg}{\Sigma}
\newcommand{\th}{\theta}\\\newcommand{\bb}{\begin{bmatrix}}
\newcommand{\eb}{\end{bmatrix}}
\newcommand{\bv}{\begin{vmatrix}}
\newcommand{\ev}{\end{vmatrix}}\\\newcommand{\im}{^{-1}}
\newcommand{\pr}{^{\prime}}
\newcommand{\ppr}{^{\prime\prime}}\end{aligned}\end{align} \]</div>
<div class="section" id="chapter-12-1-mean-variance-and-probability">
<h1>Chapter 12.1 Mean, Variance, and Probability<a class="headerlink" href="#chapter-12-1-mean-variance-and-probability" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>The <strong>mean</strong> is the <em>average value</em> or expected value.</p></li>
<li><p>The <strong>variance</strong> <span class="math notranslate nohighlight">\(\sg^2\)</span> measures the average <em>squared distance</em> from the mean <span class="math notranslate nohighlight">\(m\)</span>.</p></li>
<li><p>The <strong>probabilities</strong> of <span class="math notranslate nohighlight">\(n\)</span> different outcomes are positive numbers <span class="math notranslate nohighlight">\(p_1,\cds,p_n\)</span> adding to 1.</p></li>
</ul>
<p>The sample mean starts with <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(x_1,\cds,\x_N\)</span> from a completed trial.
Their mean is the <em>average</em> of the <span class="math notranslate nohighlight">\(N\)</span> observed samples:</p>
<p><strong>Sample mean</strong>: <span class="math notranslate nohighlight">\(\dp m=\mu=\frac{1}{N}(x_1+x_2+\cds+x_N)\)</span>.</p>
<p>The <strong>expected value of</strong> <span class="math notranslate nohighlight">\(\x\)</span> starts with the probabilities <span class="math notranslate nohighlight">\(p_1,\cds,p_n\)</span> of <span class="math notranslate nohighlight">\(x_1,\cds,x_n\)</span>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Expected value</strong>: <span class="math notranslate nohighlight">\(m=\rm{E}[x]=p_1x_1+p_2x_2+\cds+p_nx_n\)</span>.</p>
</div>
<p>This is <span class="math notranslate nohighlight">\(\p\cd\x\)</span>.
Notice that <span class="math notranslate nohighlight">\(m=\bs{E}[x]\)</span> tells us what to expect, <span class="math notranslate nohighlight">\(m=\mu\)</span> tells us what we got.</p>
<div class="section" id="variance-around-the-mean">
<h2>Variance (around the mean)<a class="headerlink" href="#variance-around-the-mean" title="Permalink to this headline">¶</a></h2>
<p>The <strong>variance</strong> <span class="math notranslate nohighlight">\(\sg^2\)</span> measures expected distance (squared) from the expected mean <span class="math notranslate nohighlight">\(\bs{E}[x]\)</span>.
The <strong>sample variance</strong> <span class="math notranslate nohighlight">\(S^2\)</span> measures actual distance (squared) from the sample mean.
The square root is the <strong>standard deviation</strong> <span class="math notranslate nohighlight">\(\sg\)</span> <strong>or</strong> <span class="math notranslate nohighlight">\(S\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Sample variance</strong>: <span class="math notranslate nohighlight">\(\dp S^2=\frac{1}{N-1}[(x_1-m)^2+\cds+(x_N-m)^2]\)</span>.</p>
</div>
<p>Statisticians divide by math:<cite>N-1</cite> and not <span class="math notranslate nohighlight">\(N\)</span> so that <span class="math notranslate nohighlight">\(S^2\)</span> is an unbiased estimate of <span class="math notranslate nohighlight">\(\sg^2\)</span>.
One degree of freedom is already accounted for in the sample mean.</p>
<p>An important identity comes from splitting each <span class="math notranslate nohighlight">\((x-m)^2\)</span> into <span class="math notranslate nohighlight">\(x^2-2mx+m^2\)</span>:</p>
<p>sum of <span class="math notranslate nohighlight">\((x_i-m)^2=(\)</span>sum of <span class="math notranslate nohighlight">\(x_i^2)-2m(\)</span>sum of <span class="math notranslate nohighlight">\(x_i)+(\)</span>sum of <span class="math notranslate nohighlight">\(m^2)\)</span></p>
<p><span class="math notranslate nohighlight">\(=(\)</span>sum of <span class="math notranslate nohighlight">\(x_i^2)-2m(Nm)+Nm^2=(\)</span>sum of <span class="math notranslate nohighlight">\(x_i^2)-Nm^2\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\sg^2=\rm{E}[(x=m)^2]=p_1(x_1-m)^2+\cds+p_n(x_n-m)^2\)</span>.</p>
</div>
</div>
<div class="section" id="continuous-probability-distributions">
<h2>Continuous Probability Distributions<a class="headerlink" href="#continuous-probability-distributions" title="Permalink to this headline">¶</a></h2>
<p><span class="math notranslate nohighlight">\(F=\)</span> <strong>integral of</strong> <span class="math notranslate nohighlight">\(p\)</span>: <strong>Probability of</strong> <span class="math notranslate nohighlight">\(\dp a\leq x\leq b=\int_a^b p(x)dx=F(b)-F(a)\)</span>.</p>
</div>
<div class="section" id="mean-and-variance-of-p-x">
<h2>Mean and Variance of <span class="math notranslate nohighlight">\(p(x)\)</span><a class="headerlink" href="#mean-and-variance-of-p-x" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Mean</strong>: <span class="math notranslate nohighlight">\(\dp m=\rm{E}[x]=\int xp(x)dx\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\dp\sg^2=\rm{E}[(x-m)^2]=\int p(x)(x-m)^2dx\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Uniform distribution for</strong> <span class="math notranslate nohighlight">\(0\leq x\leq a\)</span>; <strong>Density</strong>
<span class="math notranslate nohighlight">\(\dp p(x)=\frac{1}{a}\)</span>; <strong>Cumulative</strong> <span class="math notranslate nohighlight">\(\dp F(x)=\frac{x}{a}\)</span>:</p>
<ul class="simple">
<li><p><strong>Mean</strong>: <span class="math notranslate nohighlight">\(\dp m=\frac{a}{2}\)</span> halfway</p></li>
<li><p><strong>Variance</strong>: <span class="math notranslate nohighlight">\(\dp \sg^2=\int_0^a\frac{1}{a}\left(x-\frac{a}{2}\right)^2dx=\frac{a^2}{12}\)</span>.</p></li>
</ul>
</div>
</div>
<div class="section" id="normal-distribution-bell-shaped-curve">
<h2>Normal Distribution: Bell-shaped Curve<a class="headerlink" href="#normal-distribution-bell-shaped-curve" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Central Limit Theorem (informal)</strong>: The average of <span class="math notranslate nohighlight">\(N\)</span> samples of
“any” probability distribution approaches a normal distribution as
<span class="math notranslate nohighlight">\(N\rightarrow\infty\)</span>.</p>
</div>
<p>The standard normal distribution is symmetric around <span class="math notranslate nohighlight">\(x=0\)</span>, so its mean value is <span class="math notranslate nohighlight">\(m=0\)</span>.
It is chosen to have a standard variance <span class="math notranslate nohighlight">\(\sg^2=1\)</span>.
It is called <span class="math notranslate nohighlight">\(\bs{\rm{N}}(0,1)\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Standard normal distribution</strong>: <span class="math notranslate nohighlight">\(\dp p(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\)</span></p>
</div>
<ul class="simple">
<li><p><strong>Total probability</strong> <span class="math notranslate nohighlight">\(=1\)</span>:
<span class="math notranslate nohighlight">\(\dp \int_{-\infty}^{\infty}p(x)dx=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-x^2/2}dx=1\)</span></p></li>
<li><p><strong>Mean</strong> <span class="math notranslate nohighlight">\(\rm{E}[x]=0\)</span>: <span class="math notranslate nohighlight">\(\dp m=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}xe^{-x^2/2}dx=0\)</span></p></li>
<li><p><strong>Variance</strong> <span class="math notranslate nohighlight">\(\rm{E}[x^2]=1\)</span>: <span class="math notranslate nohighlight">\(\dp\sg^2=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}(x-0)^2e^{-x^2/2}dx=1\)</span></p></li>
</ul>
<p>The probability that a random sample falls between <span class="math notranslate nohighlight">\(-\sg\)</span> and <span class="math notranslate nohighlight">\(\sg\)</span>
is <span class="math notranslate nohighlight">\(F(\sg)-F(-\sg)\approx\frac{2}{3}\)</span>.
This is because <span class="math notranslate nohighlight">\(\int_{-\sg}^{\sg}p(x)dx\)</span> equals
<span class="math notranslate nohighlight">\(\int_{-\infty}^{\sg}p(x)dx-\int_{-\infty}^{-\sg}p(x)dx=F(\sg)-F(-\sg)\)</span>.</p>
<p>Similarly, the probability that a random <span class="math notranslate nohighlight">\(x\)</span> lies between <span class="math notranslate nohighlight">\(-2\sg\)</span>
and <span class="math notranslate nohighlight">\(2\sg\)</span> is <span class="math notranslate nohighlight">\(F(2\sg)-F(-2\sg)\approx 0.95\)</span>.</p>
<p>The normal distribution with any mean <span class="math notranslate nohighlight">\(m\)</span> and standard deviation
<span class="math notranslate nohighlight">\(\sg\)</span> comes by shifting and stretching the standard
<span class="math notranslate nohighlight">\(\bs{\rm{N}}(0,1)\)</span>.
<strong>Shift</strong> <span class="math notranslate nohighlight">\(x\)</span> <strong>to</strong> <span class="math notranslate nohighlight">\(x-m\)</span>.
<strong>Stretch</strong> <span class="math notranslate nohighlight">\(x-m\)</span> <strong>to</strong> <span class="math notranslate nohighlight">\((x-m)/\sg\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Gaussian density</strong> <span class="math notranslate nohighlight">\(p(x)\)</span>; <strong>Normal distribution</strong> <span class="math notranslate nohighlight">\(\bs{\rm{N}}(m,\sg)\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dp p(x)=\frac{1}{\sg\sqrt{2\pi}}e^{-(x-m)^2/2\sg^2}\)</span></p></li>
</ul>
</div>
<p>The integral of <span class="math notranslate nohighlight">\(p(x)\)</span> is <span class="math notranslate nohighlight">\(F(x)\)</span>–the probability that a random sample will fall below <span class="math notranslate nohighlight">\(x\)</span>.
The differential <span class="math notranslate nohighlight">\(p(x)dx=F(x+dx)-F(x)\)</span> is the probability that a random
sample will fall between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x+dx\)</span>.
There is no simple formula to integrate <span class="math notranslate nohighlight">\(e^{-x^2/2}\)</span>, so this cumulative
distribution <span class="math notranslate nohighlight">\(F(x)\)</span> is computed and tabulated very carefully.</p>
</div>
<div class="section" id="n-coin-flips-and-n-rightarrow-infty">
<h2><span class="math notranslate nohighlight">\(N\)</span> Coin Flips and <span class="math notranslate nohighlight">\(N\rightarrow \infty\)</span><a class="headerlink" href="#n-coin-flips-and-n-rightarrow-infty" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Linearity</strong>: <span class="math notranslate nohighlight">\(x_{\rm{new}}=ax_{\rm{old}}+b\)</span> has
<span class="math notranslate nohighlight">\(m_{\rm{new}}=am_{\rm{old}}+b\)</span> and
<span class="math notranslate nohighlight">\(\sg_{\rm{new}}^2=a^2\sg_{\rm{old}}^2\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Shafted and scaled</strong>: <span class="math notranslate nohighlight">\(\dp X=\frac{x-m}{\sg}=\frac{x-\frac{1}{2}N}{\sqrt{N}/2}\)</span></p>
<ul class="simple">
<li><p><strong>Subtracting</strong> <span class="math notranslate nohighlight">\(m\)</span> <strong>is “centering” or “detrending”</strong>.
<strong>The mean of</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>is zero</strong>.</p></li>
<li><p><strong>Dividing by</strong> <span class="math notranslate nohighlight">\(\sg\)</span> <strong>is “normalizing” or “standardizing”</strong>.
<strong>The variance of</strong> <span class="math notranslate nohighlight">\(X\)</span> <strong>is 1</strong>.</p></li>
</ul>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The center probability <span class="math notranslate nohighlight">\(\dp\bigg(\frac{N}{2}\)</span> heads,
<span class="math notranslate nohighlight">\(\dp\frac{N}{2}\)</span> tails<span class="math notranslate nohighlight">\(\bigg)\)</span> is
<span class="math notranslate nohighlight">\(\dp\frac{1}{2^N}\frac{N!}{(N/2)!(N/2)!}\)</span>.</p>
</div>
<p>For large <span class="math notranslate nohighlight">\(N\)</span>, Stirling’s formula <span class="math notranslate nohighlight">\(\sqrt{2\pi N}(N/e)^N\)</span> is a close approximation to <span class="math notranslate nohighlight">\(N!\)</span>/
Use Stirling for <span class="math notranslate nohighlight">\(N\)</span> and twice for <span class="math notranslate nohighlight">\(N/2\)</span>:</p>
<p><strong>Limit of coin-flip Center probability</strong>:</p>
<div class="math notranslate nohighlight">
\[p_{N/2}\approx\frac{1}{2^N}\frac{\sqrt{2\pi N}(N/e)^N}{\pi N(N/2e)^N}=
\frac{\sqrt{2}}{\sqrt{\pi N}}=\frac{1}{\sqrt{2\pi}\sg}.\]</div>
</div>
<div class="section" id="monte-carlo-estimation-methods">
<h2>Monte Carlo Estimation Methods<a class="headerlink" href="#monte-carlo-estimation-methods" title="Permalink to this headline">¶</a></h2>
<p>Applied mathematics has moved to <strong>accepting uncertainty in the inputs and estimating the variance in the outputs</strong>.
<strong>Monte Carlo method</strong> approximates an expected value <span class="math notranslate nohighlight">\(\rm{E}[x]\)</span> by a sample average <span class="math notranslate nohighlight">\((x_1+\cds+x_N)/N\)</span>.</p>
<p>Each sample comes from a set of data <span class="math notranslate nohighlight">\(b_k\)</span>.
<em>Monte Carlo randomly chooses this data</em> <span class="math notranslate nohighlight">\(b_k\)</span>, <em>it computes the outputs</em>
<span class="math notranslate nohighlight">\(x_k\)</span>, <em>and then it averages those</em> <span class="math notranslate nohighlight">\(x\)</span>’s.
Decent accuracy for <span class="math notranslate nohighlight">\(\rm{E}[x]\)</span> often requires many samples <span class="math notranslate nohighlight">\(b\)</span> and huge computing cost.
The error in approimating <span class="math notranslate nohighlight">\(\rm{E}[x]\)</span> by <span class="math notranslate nohighlight">\((x_1+\cds+x_N)/N\)</span> is normally of order <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span>.
<em>Slow improvements as</em> <span class="math notranslate nohighlight">\(N\)</span> <em>increases</em>.</p>
<p>Suppose it is much simpler to simulate another variable <span class="math notranslate nohighlight">\(y(b)\)</span> close to <span class="math notranslate nohighlight">\(x(b)\)</span>.
Then use <span class="math notranslate nohighlight">\(N\)</span> computationis of <span class="math notranslate nohighlight">\(y(b_k)\)</span> and only <span class="math notranslate nohighlight">\(N^*&lt;N\)</span>
computations of <span class="math notranslate nohighlight">\(x(b_k)\)</span> to estimate <span class="math notranslate nohighlight">\(\rm{E}[x]\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>2-level Monte Carlo</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dp \rm{E}[x]\approx\frac{1}{N}\sum_1^N y(b_k)+\frac{1}{N^*}\sum_1^{N^*}[x(b_k)-y(b_k)]\)</span>.</p></li>
</ul>
</div>
<p>The idea is that <span class="math notranslate nohighlight">\(x-y\)</span> has a smaller variance <span class="math notranslate nohighlight">\(\sg^*\)</span> than the original <span class="math notranslate nohighlight">\(x\)</span>/
Therefore <span class="math notranslate nohighlight">\(N^*\)</span> can be smaller than <span class="math notranslate nohighlight">\(N\)</span>, with the same accuracy for <span class="math notranslate nohighlight">\(\rm{E}[x]\)</span>.
We do <span class="math notranslate nohighlight">\(N\)</span> cheap simulations to find the <span class="math notranslate nohighlight">\(y\)</span>’s.
Those cost <span class="math notranslate nohighlight">\(C\)</span> each.
We only do <span class="math notranslate nohighlight">\(N^*\)</span> expensive simulations involving <span class="math notranslate nohighlight">\(x\)</span>’s.
Those cost <span class="math notranslate nohighlight">\(C^*\)</span> each.
The total computing cost is <span class="math notranslate nohighlight">\(NC+N^*C^*\)</span>.</p>
<p>Calculus minimizes the overall variance for a fixed total cost.
The optimal ratio <span class="math notranslate nohighlight">\(N^*/N\)</span> is <span class="math notranslate nohighlight">\(\sqrt{C/C^*}\sg^*/\sg\)</span>.
Three-level Monte Carlo would simulate <span class="math notranslate nohighlight">\(x,y\)</span> and <span class="math notranslate nohighlight">\(z\)</span>:</p>
<div class="math notranslate nohighlight">
\[\rm{E}[x]\approx\frac{1}{N}\sum_1^N z(b_k)+\frac{1}{N^*}\sum_1^{N^*}[y(b_k)-
z(b_k)]+\frac{1}{N^{**}}\sum_1^{N^{**}}[x(b_k)-y(b_k)].\]</div>
</div>
<div class="section" id="review-three-formulas-for-the-mean-and-the-variance">
<h2>Review: Three Formulas for the Mean and the Variance<a class="headerlink" href="#review-three-formulas-for-the-mean-and-the-variance" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic">
<li><p><strong>Samples</strong> <span class="math notranslate nohighlight">\(X_1\)</span> <strong>to</strong> <span class="math notranslate nohighlight">\(X_N\)</span>:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dp m=\frac{X_1+\cds+X_N}{N}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dp S^2=\frac{(X_1-m)^2+\cds+(X_N-m)^2}{N-1}\)</span></p></li>
</ul>
</div></blockquote>
</li>
<li><p><span class="math notranslate nohighlight">\(n\)</span> <strong>possible outputs with probabilities</strong> <span class="math notranslate nohighlight">\(p_i\)</span>:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dp m=\sum_1^np_ix_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dp \sg^2=\sum_1^np_i(x_i-m)^2\)</span></p></li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Range of outputs with probability density</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dp m=\int xp(x)dx\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\dp \sg^2=\int(x-m)^2p(x)dx\)</span></p></li>
</ul>
</div></blockquote>
</li>
</ol>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="chap12-2.html" class="btn btn-neutral float-right" title="Chapter 12.2 Covariance Matrices and Joint Probabilities" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="index12.html" class="btn btn-neutral float-left" title="Chapter 12 Linear Algebra in Probability &amp; Statistics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Ziniu Yu.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>